---
title: "Documentación Proyecto final"
author: "Felipe Gerard, Omar Díaz, Andrés Villaseñor"
date: "21 de mayo de 2015"
output: html_document
---

Limpieza de la colección de abstracts
===================================================

0. Descripción del proceso
----------------------------

La limpieza de los datos consiste en los siguientes pasos:

1. Conversión de archivos de texto a JSON estructurado en _bash_ (`abstract2json.sh`):

```{r, eval=FALSE}
    {
  "1":{
	    "Titulo":{"Ejemplo de título"},
	    "Fecha":{"2013-01-02"},
	    "Abstract":{"Ejemplo de abstract"},
	    ...
	    },
	"2":{
	    "Titulo":{"Ejemplo de otro título"},
	    "Fecha":{"2003-03-02"},
	    "Abstract":{"Ejemplo de otro abstract"},
	    ...
	    },
	...
    }
```

2. Conversión de JSON estructurado a data.frame de R para el análisis en _R_ (`json2dataframe_abstracts.R`):


1. Texto --> JSON
--------------------------------

El primer paso es sustituir los signos de puntuación y los saltos de línea en caracteres manejables. El fin de esto es que no afecten los JSON ni generen resultados raros. Usando los comandos `tr` y `sed` convertimos los saltos de línea en " <br> ", los puntos por "<punto>", etc.

Ya con la puntuación resuelta, pasamos los txt a JSON. Los archivos originales ya venían en un formato razonable similar a JSON, "nombre_de_atributo:descripción", así que lo pudimos convertir sin problema agregando las comillas pertinentes, etc. Hubo que tener cuidado con el encoding de los textos y con el hecho de que el formato no era 100% formal. Para esta parte utilizamos sobre todo `sed` y `awk`.


2. JSON --> data.frame
---------------------------------------

Primero nos aseguramos de que el formato sea legible en _R_. Cuando no, utilizamos _Python_ para limpiar el formato. Una vez habiendo leído la lista a _R_ con la librería `jsonlite`, convertimos la lista recursiva a `data.frame`. Para ahorrar tiempo, utilizamos la función `mclapply` del paquete `parallel`. Finalmente, regresamos la puntuación a su forma original para que el texto recuperado sea legible.


Ejemplo
----------

### 1. Texto original:


```{r, eval=FALSE}
Title       : RFLP Patterns as a Measure of Diversity in Small Populations
Type        : Award
NSF Org     : MCB
Latest
Amendment
Date        : May 31,  1990
File        : a9000031

Award Number: 9000031
Award Instr.: Standard Grant
Prgm Manager: Maryanna P. Henkart
MCB  DIV OF MOLECULAR AND CELLULAR BIOSCIENCE
BIO  DIRECT FOR BIOLOGICAL SCIENCES
    Start Date  : June 1,  1990
Expires     : May 31,  1994        (Estimated)
    Expected
    Total Amt.  : $300000             (Estimated)
Investigator: Marcia M. Miller mamiller@coh.org  (Principal Investigator current)
    Sponsor     : Beckman Res Inst Cty Hope
    1500 E. Duarte Road
    Duarte, CA  910103000    /   -

    NSF Program : 1114      CELL BIOLOGY
    Fld Applictn: 0000099   Other Applications NEC
    61        Life Science Biological
    Program Ref : 9285,
    Abstract    :

    Studies of chickens have provided serological and nucleic acid
    probes useful in defining the major histocompatibility complex
    (MHC) in other avian species.  Methods used in detecting genetic
    diversity at loci within the MHC of chickens and mammals will be
    applied to determining the extent of MHC polymorphism within
    small populations of ring-necked pheasants, wild turkeys, cranes,
    Andean condors and other species.  The knowledge and expertise
    gained from working with the MHC of the chicken should make for
    rapid progress in defining the polymorphism of the MHC in these
    species and in detecting the polymorphism of MHC gene pool within
    small wild and captive populations of these birds.

    Genes within the major histocompatibility complex (MHC) are known
    to encode molecules that provide the context for recognition of
    foreign antigens by the immune system.  Whether a given animal is
    able to mount an immune response to the challenge of a pathogen
    is determined, in part, by the allelic makeup of its MHC.  In
    many species, an unusually high degree of polymorphism is
    maintained at multiple loci within the MHC in freely breeding
    populations.  The allelic pool within a population presumably
    provides diversity upon which to draw in the face of
    environmental challenge.  The objective of the proposed research
    is to extend ongoing studies of the MHC of domesticated fowl to
    include avian species experiencing severe reduction in population
    size.  Knowledge of the MHC gene pool within populations and of
    the haplotypes of individual animals may be useful in the
    husbandry of species requiring intervention for their
    preservation.
```

### 2. JSON

```{r, eval=FALSE}
"51758":{
    "Title":"RFLP Patterns as a Measure of Diversity in Small Populations",
    "Date":"May 31<coma> 1990",
    "Award Number":"9000031",
    "Investigator":"Marcia M<punto> Miller mamillercoh<punto>org <abre_parent>Principal 
    Investigator current<cierra_parent>",
    "Sponsor":"Beckman Res Inst Cty Hope<br> 1500 E<punto> Duarte Road<br> Duarte<coma> 
    CA 910103000 <diagonal> <guion><br>",
    "Fld Applictn":"0000099 Other Applications NEC <br> 61 Life Science Biological",
    "Abstract":"<br> <br> Studies of chickens have provided serological and nucleic 
    acid <br> probes useful in defining the major histocompatibility complex <br> 
    <abre_parent> MHC<cierra_parent> in other avian species<punto> Methods used in 
    detecting genetic <br> diversity at loci within the MHC of chickens and mammals will
    be <br> applied to determining the extent of MHC polymorphism within <br> small 
    populations of ring<guion>necked pheasants <coma> wild turkeys<coma> cranes<coma> <br> 
    Andean condors and other species<punto> The knowledge and expertise <br> gained from 
    working with the MHC of the chicken should make for <br> rapid progress in defining 
    the polymorphism of the MHC in these <br> species and in detecting the polymorphism 
    of MHC gene pool within <br> small wild and captive populations of
    these birds<punto> <br> <br> Genes within the major histocompatibility complex 
    <abre_parent> MHC<cierra_parent> are known <br> to encode molecules that provide 
    the context for recognition of <br> foreign antigens by the immune system<punto> 
    Whether a given animal is <br> able to mount an immune response to the challenge 
    of a pathogen <br> is determined<coma> in part<coma> by the allelic makeup of its 
    MHC<punto> In <br> many species<coma> an unusually high degree of polymorphism 
    is <br> maintained at multiple loci within the MHC in freely breeding <br> 
    populations<punto> The allelic pool within a population presumably <br>  
    provides diversity upon  which to draw in the face of <br> 
    environmental challenge<punto> The objective of the proposed research <br> 
    is to extend ongoing studies of the MHC of domesticated fowl to <br> 
    include avian species experiencing severe reduction in population <br> 
    size<punto> Knowledge ofthe MHC gene pool within populations and of <br> 
    the haplotypes of individual animals may be useful in the <br> 
    husbandry of species requiring intervention for their <br> 
    preservation<punto><br>"
}
```

### 3. data.frame

Tiene las columnas del JSON anterior pero en versión y la puntuación en formato normal. No incluimos el ejemplo porque el formato no es práctico.



### 4. Implementación del modelo

Una vez con los datos limpios como se mencionó anteriormente, procedemos a implementar el modelo, para esto tenemos lo siguiente.


```{r, echo=FALSE}

#cargamos los datos
load('/Users/lechuga/data-science/metodos-analiticos/omar/Abstracts/abstracts_clean.Rdata')
load('/Users/lechuga/data-science/metodos-analiticos/andres/corpus.Rdata')
```



```{r,warning=FALSE,message=FALSE}
#Las librerías utilizadas son las siguientes:
library(Matrix)
library(dplyr)
library(tm)
library(slam)
library(Rstem)
library(ggplot2)
library(wordcloud)
library(knitr)

#carga de datos
#load('abstracts_clean.Rdata')
abstracts2 <- as.data.frame(abstracts2)


########################################################  Query #######################################################

query <- 'Polarity Transistion Studies in the Waianae Volcano: From the Gilbert-Gauss to<br> the Upper Kaena Reversals'
query <- 'Markov Chain Monte Carlo'
query  <- 'MCMC'

#limpieza del query
query.limp <- Corpus(VectorSource(query))
query.limp <- tm_map(query.limp,function(x){
  q1 <- gsub('[-]|<br>',' ',x)
  gsub('[()]|[.,;:`"*#&/><]|[\\\']|[]\\[]','',q1)
})
query.limp <- tm_map(query.limp,removeWords,stopwords("english"))
query.limp <- tm_map(query.limp, function(x) stripWhitespace(x) %>% tolower)
query.limp <- tm_map(query.limp,function(x){
  z <- strsplit(x, " +")[[1]]
  z.stem <- wordStem(z, language="english")
  PlainTextDocument(paste(z.stem, collapse=" "))
})


```

```{r, eval=FALSE}
###################################  limpieza de la informacion y creacion del corpus (datos) ##################################

# filtramos cosas feas en abstract y title
d <- abstracts2 %>%
  filter(grepl('Presidential Awardee',Title)=='FALSE') %>% #815
  filter(grepl('Not Available',Abstract)=='FALSE') %>% #1267
  filter(Title != '') %>% #8
  filter(Abstract != '' ) %>% #2180
  filter(grepl('-----------------------------------------------------------------------',Abstract)=='FALSE') %>%
  mutate(id=row_number()) %>%
  dplyr::select(id,Title,Abstract,Fld.Applictn,Date,Sponsor,Investigator,Award.Number)

d_fin <- d

```

Posteriormente creamos la matriz de términos-documentos y tomamos los pesos utilizano como criterio __"ntc"__ ya que observamos que es el que mejor resultado genera. 

```{r}
#creamos la matriz terminos documentos  
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(3, Inf)))
colnames(tdm.1) <- seq(1,tdm.1$ncol)

#eliminamos los documentos que no tienen terminos (empty docs) 
#a través de la suma de las columnas
idx_sum <- as.numeric(as.matrix(rollup(tdm.1, 1, na.rm=TRUE, FUN = sum)))
tdm_new <- tdm.1[,idx_sum>0]

#actualizamos los pesos
tdm.2 <- weightSMART(tdm_new, spec = 'ntc')
#revisamos la normalización de los pesos
head(sort(vec.1 <- as.matrix(tdm.2[,500]),dec=T))

```



Ya con el modelo listo, necesitamos hacer queries a la _"base de datos"_ por lo que tenemos que homogenizar el query con los datos, es deicr tenemos que transformarlos a una matriz de términos documentos de la siguiente forma.


```{r}

query <- 'The main objective of this proposal is better understanding of underlying atomic<br> and  molecular processes involved in the microstructural evolution of<br> polycrytalline TiS2 and other powders synthesized using the thi-sol-gel process<br> previously proposed by the P.I. A secondary objective is to investigate the<br> electrochemical properties of the resulted thin layer, the influence of heat<br> treatment and of charging on the particulate microstructure. The effects of<br> particle size, shape, crystallite size, orientation and stoichiometry on the<br> elctrochemical properties and the mechanisms involved in the evolution of the<br> microstructure will be investigated using differential thermal and<br> thermogravimetric analyses, scanning and transmission electron microscopy. <br> If successful, the research may provide a method to synthesize an engineered<br> microstructure with optimal electrochemical properties. Applications include<br> electrochemical batteries. The preliminary test have indicated a dramatic<br> increase of the number of charging cycles for a given reduction in performance'
#limpieza del query
query.l <- Corpus(VectorSource(query))
q.1 <- tm_map(query.l,function(x){
  q1 <- gsub('[-]|<br>',' ',x)
  gsub('[()]|[.,;:`"*#&/><]|[\\\']|[]\\[]','',q1)
})
q.2 <- tm_map(q.1,removeWords,stopwords("english"))
q.2 <- tm_map(q.2, function(x) stripWhitespace(x) %>% tolower)
q.2 <- tm_map(q.2,function(x){
  z <- strsplit(x, " +")[[1]]
  z.stem <- wordStem(z, language="english")
  PlainTextDocument(paste(z.stem, collapse=" "))
})
query.limp <- q.2

```


Ahora tenemos que multiplicar la base (la matriz rala de todos los términos documentos) por la matriz obtenida anteriormente. Esto ya que nos permite quedarnos con los términos que aportan más al score TF-IDF y nos permiten medir la distancia en este caso en particular utilizando la __"distancia coseno"__, ya que el producto punto dividido entre la logitud de estas matrices es precisamente el tamaño de la intersección entre estos dos _conjuntos_. Por lo tanto este calculo nos permite conocer el coseno del angulo entre estos dos _"vectores"_



```{r}

# Multiplicacion query * tdm 
mat.1 <- sparseMatrix(i=tdm.2$i, j=tdm.2$j, x = tdm.2$v)
dictionary <- tdm.2$dimnames$Terms
query.vec.1 <- TermDocumentMatrix(query.limp, 
                                  control = list(dictionary = dictionary,
                                                 wordLengths=c(1, Inf))) 

#normalizar con ntc el query
query.vec.norm <- as.matrix(query.vec.1)/sqrt(sum(query.vec.1^2)) 

aa <- t(mat.1)%*%query.vec.norm
```

Ahora vislualizamos los 15 resultados que más aportarón información al modelo

```{r}
idx_top <- order(aa, decreasing=T)
out <- d[idx_top,] %>%
  select(Title, Date, Sponsor, Abstract) %>%
  cbind(score = sort(aa,decreasing = T)) %>%
  filter(score > 0)
res <- out %>% head(15)
res$Abstract <- gsub('<br>','',res$Abstract)
res$Title <- gsub('<br>','',res$Title)
res$Sponsor <- gsub('<br>','',res$Sponsor)

kable(res$Title)
```


* Observamos el histograma de discriminación, el cual nos da una idea gráfica e intuitiva de como es que estamos haciendo nuestro criterio de selección, el cual se encuentra a continuación.


```{r}
m <- data.frame(min=min(res$score))
# Estadísticas. Son sobresalientes las palabras que mostramos?
ggplot() +
  geom_bar(data=out, mapping=aes(x=score)) +
  geom_vline(data=res, aes(xintercept=min(score)), color='red')
```


A la derecha de la línea roja, se marca la cantidad de documentos estamos considerando para hacer la recuperación de información. Si la cantidad de documentos a la derecha de la linea roja es muy grande quiere deicr que no estamos logrando una buena discriminación, de lo contrario tenemos que hay pocos documentos que nos aportan una ganancia de información, es decir, tenemos que la información es más valiosa y por lo tanto el resultado.

A continuación, vemos como obtenemos de forma particular las palabras que más contribuyen a nuestro modelo.

```{r}
best <- function(nmatch = 3, nterm = 5){
  v.q <- query.vec.norm
  outlist <- list()
  for(i in 1:nmatch){
    #colnames(tdm.2)[idx_top[i]]
    #idx_top[nmatch]
    v.j <- mat.1[,idx_top[i]]
    v <- v.j*v.q
    #length(v)
    top_contrib <- order(v, decreasing = T)
    outlist[[i]] <- data.frame(term=dictionary[top_contrib[1:nterm]], # tdm.2$dimnames$Terms[top_contrib[1]]
                               score_contrib=v[top_contrib[1:nterm]], stringsAsFactors=F) %>%
      filter(score_contrib > 0) %>%
      data.frame(rank = i, match = colnames(tdm.2)[idx_top[i]], total_score = sum(v), stringsAsFactors = F)
  }
  rbind_all(outlist)[c(3,4,5,1,2)] %>%
    group_by(term) %>%
    summarise(contrib=sum(score_contrib)) %>%
    arrange(desc(contrib))
}

best <- best(nmatch = 15, nterm = nrow(unique(as.data.frame(strsplit(as.character(query[[1]])," ")[[1]]))))

kable(head(best,15))
```

Finalmente el wordcloud para una visualización mas amigable.

```{r, fig.width=7, fig.height=7, warning=FALSE}
wordcloud(best$term,best$contrib,
          scale=c(5,.7),
          min.freq=0.1,
          ordered.colors=T,
          colors=colorRampPalette(brewer.pal(9,"Set1"))(nrow(best)))
```


