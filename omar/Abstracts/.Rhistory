corp.1 <- tm_map(corpus.frases,  function(x){
gsub('--|[«»\\\',;:".!¡¿?\\(\\)\\[\\]&0-9\\*#/]|<br>','',x) # Checar escapes de &*[]
})
corp.2 <- tm_map(corp.1, function(x) stripWhitespace(x) %>% tolower %>% PlainTextDocument)
############################  Creacion de la matriz terminos documentos y los pesos por ntc ############################
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(1, Inf)))
tdm.2 <- weightSMART(tdm.1, spec = 'ntc')
colnames(tdm.2) <- paste(d$Word, d$id, sep='_')
#####################################  revisamos la normalizacion de los pesos #########################################
head(sort(vec.1 <- as.matrix(tdm.2[,'action_1']),dec=T))
########################################################  Query #######################################################
query <- 'i would like to go to the green park and spend a great time with my friends'
query <- 'wine'
query <- 'corazón'
d[which(d$Word %in% query)[1],]$Def
d[which(d$Word %in% query)[1],]$Def==NA
if(d[which(d$Word %in% query)[1],]$Def==NA){a <- 3}
if(is.na(d[which(d$Word %in% query)[1],]$Def)){a <- 3}
a
query <- 'corazón'
if(sapply(gregexpr("\\W+", query), length) + 1 == 2) {
#si la longitud del query es de 1 entonces, buscamos y limpiamos su definicion
if(is.na(d[which(d$Word %in% query)[1],]$Def)){
aux <- tm_map(Corpus(VectorSource(query)),removeWords,stopwords("english"))
}else{
definicion <- paste(query,d[which(d$Word %in% query)[1],]$Def)
definicion <- gsub('--|[],;:.[]|<br>|[()«»"#*`¿?¡!/&%$=]','',definicion)
definicion <- tm_map(Corpus(VectorSource(definicion)), function(x) stripWhitespace(x) %>% tolower %>% PlainTextDocument)
definicion <- tm_map(definicion,removeWords,stopwords("english"))
aux <- definicion
}
}else{
aux <- tm_map(Corpus(VectorSource(query)),removeWords,stopwords("english"))
}
aux[[1]]
query <- 'car'
if(sapply(gregexpr("\\W+", query), length) + 1 == 2) {
#si la longitud del query es de 1 entonces, buscamos y limpiamos su definicion
if(is.na(d[which(d$Word %in% query)[1],]$Def)){
aux <- tm_map(Corpus(VectorSource(query)),removeWords,stopwords("english"))
}else{
definicion <- paste(query,d[which(d$Word %in% query)[1],]$Def)
definicion <- gsub('--|[],;:.[]|<br>|[()«»"#*`¿?¡!/&%$=]','',definicion)
definicion <- tm_map(Corpus(VectorSource(definicion)), function(x) stripWhitespace(x) %>% tolower %>% PlainTextDocument)
definicion <- tm_map(definicion,removeWords,stopwords("english"))
aux <- definicion
}
}else{
aux <- tm_map(Corpus(VectorSource(query)),removeWords,stopwords("english"))
}
aux[[1]]
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
query <- 'car'
if(sapply(gregexpr("\\W+", query), length) + 1 == 2) {
#si la longitud del query es de 1 entonces, buscamos y limpiamos su definicion
if(is.na(d[which(d$Word %in% query)[1],]$Def)){
aux <- tm_map(Corpus(VectorSource(query)),removeWords,stopwords("english"))
}else{
definicion <- paste(query,d[which(d$Word %in% query)[1],]$Def)
definicion <- gsub('--|[],;:.[]|<br>|[()«»"#*`¿?¡!/&%$=]','',definicion)
definicion <- tm_map(Corpus(VectorSource(definicion)), function(x) stripWhitespace(x) %>% tolower %>% PlainTextDocument)
definicion <- tm_map(definicion,removeWords,stopwords("english"))
aux <- definicion
}
}else{
aux <- tm_map(Corpus(VectorSource(query)),removeWords,stopwords("english"))
}
################################################ Multiplicacion query * tdm ############################################
mat.1 <- sparseMatrix(i=tdm.2$i, j=tdm.2$j, x = tdm.2$v)
dictionary <- tdm.2$dimnames$Terms
query.vec.1 <- TermDocumentMatrix(aux,
control = list(dictionary = dictionary,
wordLengths=c(1, Inf)))
query.vec.norm <- as.matrix(query.vec.1)/sqrt(sum(query.vec.1^2))
aa <- t(mat.1)%*%query.vec.norm
###################################################  top 15 palabras ##################################################
idx_top <- order(aa, decreasing=T)
out <- d[idx_top,] %>%
select(Word, id, Def) %>%
cbind(score = sort(aa,decreasing = T)) %>%
filter(score > 0)
res <- out %>% head(15)
res$Def <- gsub('<br>','',res$Def)
View(res)
##############################################  histograma de discriminacion ##########################################
m <- data.frame(min=min(res$score))
# Estadísticas. Son sobresalientes las palabras que mostramos?
ggplot() +
geom_bar(data=out, mapping=aes(x=score)) +
geom_vline(data=res, aes(xintercept=min(score)), color='red')
############################################### contribucion de las palabras #########################################
best <- function(nmatch = 3, nterm = 5){
v.q <- query.vec.norm
outlist <- list()
for(i in 1:nmatch){
#colnames(tdm.2)[idx_top[i]]
#idx_top[nmatch]
v.j <- mat.1[,idx_top[i]]
v <- v.j*v.q
#length(v)
top_contrib <- order(v, decreasing = T)
outlist[[i]] <- data.frame(term=dictionary[top_contrib[1:nterm]], # tdm.2$dimnames$Terms[top_contrib[1]]
score_contrib=v[top_contrib[1:nterm]], stringsAsFactors=F) %>%
filter(score_contrib > 0) %>%
data.frame(rank = i, match = colnames(tdm.2)[idx_top[i]], total_score = sum(v), stringsAsFactors = F)
}
rbind_all(outlist)[c(3,4,5,1,2)] %>%
group_by(term) %>%
summarise(contrib=sum(score_contrib)) %>%
arrange(desc(contrib))
}
best <- best(nmatch = 15, nterm = nrow(unique(as.data.frame(strsplit(as.character(aux[[1]])," ")[[1]]))))
########################################################  wordcloud ##################################################
wordcloud(best$term,best$contrib,
scale=c(5,.7),
min.freq=0.1,
ordered.colors=T,
colors=colorRampPalette(brewer.pal(9,"Set1"))(nrow(best)))
best
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
best
best$contrib
b <- as.data.frame(best$contrib)
b
norm(b)
b/max(b)
b/max(b)*10
library(Matrix)
library(dplyr)
library(tm)
#library(Rstem)
library(ggplot2)
library(wordcloud)
setwd("~/Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Abstracts")
load('abstracts_clean.Rdata')
abstracts2 <- as.data.frame(abstracts2)
d <- abstracts2 %>%
filter(grepl('Presidential Awardee',Title)=='FALSE') %>% #815
filter(grepl('Not Available',Abstract)=='FALSE') %>% #1267
filter(Title != '') %>% #8
filter(Abstract != '' ) %>% #2180
filter(grepl('-----------------------------------------------------------------------',Abstract)=='FALSE')
# consultas de caractres especiales en la info
#d1 <- d %>%
#  mutate(id=row_number()) %>%
#  select(id,Abstract)
#v <- filter(d1,grepl('S u m',Abstract)=='TRUE')
#View(v)
#subset(v,id==4550)
# creamos el corpus y limpiamos caracteres especiales
corpus.frases <- Corpus(VectorSource(d$Abstract))
#corpus.frases
corp.1 <- tm_map(corpus.frases,function(x){
gsub('R o o t E n t<br> r y','Root Entry',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('C o m p O b j','Comp Obj',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('S u m m a r y I n f o r m a t i o n','Summary Information',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('<br> b <br>','',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('W o r d D o c u m e n t','Word Document',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('O b j e c t P o o l','Object Pool',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('[-]|<br>',' ',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('[()]|[.,;:`"*#&/><]|[\\\']|[]\\[]','',x)
})
corp.2 <- tm_map(corp.1,removeWords,stopwords("english"))
corp.2 <- tm_map(corp.2, function(x) stripWhitespace(x) %>% tolower %>% PlainTextDocument)
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(1, Inf)))
tdm.1
tdm.2 <- weightSMART(tdm.1, spec = 'ntc')
rowTotals <- apply(tdm , 2, sum) #Find the sum of words in each Document
rowTotals <- apply(tdm.1, 2, sum) #Find the sum of words in each Document
ncol(tdm.1)
rowTotals <- apply(tdm.1, 2, sum)
nrow(tdm.1)
sum(tdm.1[,1])
for(i in 1:ncol(tdm.1)){
colTotals <- sum(tdm.1[,1])
}
for(i in 1:ncol(tdm.1)){
colTotals <- sum(tdm.1[,i])
}
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(2, Inf)))
tdm.2 <- weightSMART(tdm.1, spec = 'ntc')
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(3, Inf)))
tdm.2 <- weightSMART(tdm.1, spec = 'ntc')
load('abstracts_clean.Rdata')
abstracts2 <- as.data.frame(abstracts2)
d <- abstracts2 %>%
filter(grepl('Presidential Awardee',Title)=='FALSE') %>% #815
filter(grepl('Not Available',Abstract)=='FALSE') %>% #1267
filter(Title != '') %>% #8
filter(Abstract != '' ) %>% #2180
filter(grepl('-----------------------------------------------------------------------',Abstract)=='FALSE')
corpus.frases <- Corpus(VectorSource(d$Abstract))
corp.1 <- tm_map(corpus.frases,function(x){
gsub('R o o t E n t<br> r y','Root Entry',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('C o m p O b j','Comp Obj',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('S u m m a r y I n f o r m a t i o n','Summary Information',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('<br> b <br>','',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('W o r d D o c u m e n t','Word Document',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('O b j e c t P o o l','Object Pool',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('[-]|<br>',' ',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('[()]|[.,;:`"*#&/><]|[\\\']|[]\\[]','',x)
})
corp.2 <- tm_map(corp.1,removeWords,stopwords("english"))
corp.2 <- tm_map(corp.2, function(x) stripWhitespace(x) %>% tolower %>% PlainTextDocument)
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(3, Inf)))
tdm.2 <- weightSMART(tdm.1, spec = 'ntc')
rnorm(n = 59,mean=0,sd = 1)
x <- cbind(rnorm(n = 59,mean=0,sd = 1),rnorm(n = 59,mean=0,sd = 1),rnorm(n = 59,mean=0,sd = 1))
head(x)
colSums(x)
colSums(tdm.1)
colSums(as.matrix(tdm.1))
t <- as.matrix(tdm.1)
t <- as.data.frame(tdm.1)
tdm.2 <- weightSMART(tdm.1, spec = 'ntc')
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(3, Inf)))
for(i in 1:5){
colTotals <- sum(tdm.1[,i])
}
colTotals
for(i in 1:5){
colTotals[i] <- sum(tdm.1[,i])
}
colTotals
colTotals/nrow(tdm.1)
for(i in 1:15){
colTotals[i] <- sum(tdm.1[,i])
}
colTotals/nrow(tdm.1)
min(colTotals/nrow(tdm.1))
for(i in 1:50){
colTotals[i] <- sum(tdm.1[,i])
}
min(colTotals/nrow(tdm.1))
tdm.2 <- removeSparseTerms(tdm.1, 0.00001)
tdm.2 <- weightSMART(tdm.2, spec = 'ntc')
tdm.2 <- removeSparseTerms(tdm.1, 0.00001)
tdm.3 <- weightSMART(tdm.2, spec = 'ntc')
tdm.2 <- removeSparseTerms(tdm.1, 0.1)
tdm.3 <- weightSMART(tdm.2, spec = 'ntc')
tdm.2 <- removeSparseTerms(tdm.1, 0.8)
tdm.3 <- weightSMART(tdm.2, spec = 'ntc')
tdm.2 <- removeSparseTerms(tdm.1, 0.1)
tdm.2 <- removeSparseTerms(tdm.1, 1)
tdm.2 <- removeSparseTerms(tdm.1, .9)
tdm.3 <- weightSMART(tdm.2, spec = 'ntc')
tdm.2 <- removeSparseTerms(tdm.1, .1)
tdm.3 <- weightSMART(tdm.2, spec = 'ntc')
for(i in 1:5){
colsum[i] <- sum(tdm.2[,i])
}
colsum <- NULL
for(i in 1:5){
colsum[i] <- sum(tdm.2[,i])
}
colsum
for(i in 1:5){
colsum[i] <- sum(tdm.2[,i])
}
colsum
sum(tdm.2[,1])
sum(tdm.1[,1])
for(i in 1:5){
colsum[i] <- sum(tdm.1[,i])
}
colsum
ncol(tdm.1)
tdm.2 <- removeSparseTerms(tdm.1, .000001)
sum(tdm.1[,1])
tdm.1[,colsum>8]
tdm.3 <- tdm.1[,colsum>8]
tdm.3
tdm.1
colsum>8
sample(1:10, 5, replace=T)
x <- cbind(sample(1:10, 5, replace=T), sample(1:10, 5, replace=T), sample(1:10, 5, replace=T))
x
colSums(x)
x[,colsum>18]
x <- cbind(sample(1:10, 5, replace=T),
sample(1:10, 5, replace=T),
sample(1:10, 5, replace=T),
sample(1:10, 5, replace=T),
sample(1:10, 5, replace=T))
x
colSums(x)
x[,colsum>20]
x1 <- x[,colsum>20]
colSums(x1)
idx <- apply(tdm.1,2,sum)
install.packages("slam")
library(slam)
x <- matrix(c(1, 0, 0, 2, 1, NA), nrow = 2,
dimnames = list(A = 1:2, B = 1:3))
x
apply(x, 1L, sum, na.rm = TRUE)
rollup(x, 2L, na.rm = TRUE)
rollup(x, 2L, c(1,2,1), na.rm = TRUE)
rollup(x, 2L, c(1,NA,1), na.rm = TRUE)
x
apply(x, 2L, sum, na.rm = TRUE)
tdm.new <- apply(tdm.1, 2L, sum, na.rm = TRUE)
rollup(x, 2L, na.rm = TRUE)
z <- rollup(x, 2L, na.rm = TRUE)
z
z <- rollup(x, 1L, na.rm = TRUE)
z
class(z)
as.numeric(z)
z1 <- as.numeric(z)
z1[2]
x[,z1>1]
x
x[,z1>1]
x <- cbind(sample(1:10, 5, replace=T),
sample(1:10, 5, replace=T),
sample(1:10, 5, replace=T),
sample(1:10, 5, replace=T),
sample(1:10, 5, replace=T))
x
x <- as.matrix(x)
x
z <- rollup(x, 1, na.rm = TRUE)
z
colSums(x)
as.numeric(z)
x[,z1>30]
z1 <- as.numeric(z)
z1
x <- as.matrix(x)
x
z <- rollup(x, 1, na.rm = TRUE)
z1 <- as.numeric(z)
z1
x[,z1>30]
colSums(x[,z1>30])
data("crude")
library(Matrix)
library(dplyr)
library(tm)
library(slam)
library(ggplot2)
library(wordcloud)
data("crude")
tdm_crude <- TermDocumentMatrix(crude)
tdm_crude
idx_sum <- as.numeric(rollup(tdm_crude, 1, na.rm=TRUE, FUN = sum))
idx_sum <- rollup(tdm_crude, 1, na.rm=TRUE, FUN = sum)
idx_sum
as.matrix(idx_sum)
sum(tdm_crude[,1])
idx_sum <- rollup(tdm.1, 1, na.rm=TRUE, FUN = sum)
idx_sum <- rollup(tdm_crude, 1, na.rm=TRUE, FUN = sum)
as.matrix(idx_sum)
as.numeric(as.matrix(idx_sum))
idx_sum <- as.numeric(as.matrix(rollup(tdm.1, 1, na.rm=TRUE, FUN = sum)))
idx_sum <- rollup(tdm_crude, 1, na.rm=TRUE, FUN = sum)
idx_sum <- as.numeric(as.matrix(idx_sum))
tdm_crude_new <- tdm_crude[,idx_xum>100]
tdm_crude_new <- tdm_crude[,idx_sum>100]
tdm_crude_new
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(3, Inf)))
#eliminamos los documentos que no tienen terminos (empty docs)
idx_sum <- as.numeric(as.matrix(rollup(tdm.1, 1, na.rm=TRUE, FUN = sum)))
tdm_new <- tdm.1[,idx_sum>0]
tdm.1
tdm_new
tdm.3 <- weightSMART(tdm_new, spec = 'ntc')
tdm.3 <- removeSparseTerms(tdm_new, .9)
tdm.new
tdm.2 <- removeSparseTerms(tdm_new, .9)
tdm.2 <- removeSparseTerms(tdm_new, .9)
tdm_new
tdm.2
tdm.2 <- removeSparseTerms(tdm_new, .000001)
tdm_new
tdm.2
tdm.2 <- removeSparseTerms(tdm_new, .1)
tdm_new
tdm.2
tdm.2 <- removeSparseTerms(tdm_new, .9)
tdm_new
tdm.2
tdm.2 <- removeSparseTerms(tdm_new, .9999999)
tdm_new
tdm.2
tdm.2 <- removeSparseTerms(tdm_new, .99999)
tdm_new
tdm.2
tdm.2 <- removeSparseTerms(tdm_new, .9999)
tdm_new
tdm.2
#creamos la matriz terminos documentos
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(3, Inf)))
#eliminamos los documentos que no tienen terminos (empty docs)
idx_sum <- as.numeric(as.matrix(rollup(tdm.1, 1, na.rm=TRUE, FUN = sum)))
tdm_new <- tdm.1[,idx_sum>0]
#actualizamos los pesos
tdm.2 <- weightSMART(tdm_new, spec = 'ntc')
head(sort(vec.1 <- as.matrix(tdm.2[,500]),dec=T))
install.packages("Rstem")
install.packages("Rstem", repos = "http://www.omegahat.org/R", type="source")
library(Rstem)
corp.3 <- tm_map(corp.2,function(x){
wordStem(x,language="english")
})
corp.3 <- tm_map(corp.2,function(x){
z <- strsplit(x, " +")[[1]]
z.stem <- wordStem(z,language="english")
PlainTextDocument(paste(z.stem,collapse=" "))
})
corp.3$content[4550]
corp.3 <- tm_map(corp.2,function(x){
strsplit(x, " +")[[1]]
})
x <- 'todo esto esta muy mal'
strsplit(x, " +")
strsplit(x, " ")
strsplit(x, " ")[[1]]
corp.3 <- tm_map(corp.2,function(x){
strsplit(x, " ")[[1]]
})
corp.2[1]
corp.2[[1]]
corp.2[[10]]
corp.2$content[10]
corp.1 <- tm_map(corpus.frases,function(x){
gsub('R o o t E n t<br> r y','Root Entry',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('C o m p O b j','Comp Obj',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('S u m m a r y I n f o r m a t i o n','Summary Information',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('<br> b <br>','',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('W o r d D o c u m e n t','Word Document',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('O b j e c t P o o l','Object Pool',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('[-]|<br>',' ',x)
})
corp.1 <- tm_map(corp.1,function(x){
gsub('[()]|[.,;:`"*#&/><]|[\\\']|[]\\[]','',x)
})
corp.2 <- tm_map(corp.1,removeWords,stopwords("english"))
corp.2$content[10]
corp.2 <- tm_map(corp.2,function(x){
z <- strsplit(x, " +")[[1]]
z.stem <- wordStem(z, language="english")
PlainTextDocument(paste(z.stem, collapse=" "))
})
corp.2 <- tm_map(corp.1,removeWords,stopwords("english"))
corp.2 <- tm_map(corp.2, function(x) stripWhitespace(x) %>% tolower)
corp.2 <- tm_map(corp.2,function(x){
z <- strsplit(x, " +")[[1]]
z.stem <- wordStem(z, language="english")
PlainTextDocument(paste(z.stem, collapse=" "))
})
corp.2$content[10]
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(3, Inf)))
#eliminamos los documentos que no tienen terminos (empty docs)
idx_sum <- as.numeric(as.matrix(rollup(tdm.1, 1, na.rm=TRUE, FUN = sum)))
tdm_new <- tdm.1[,idx_sum>0]
tdm.2 <- weightSMART(tdm_new, spec = 'ntc')
head(sort(vec.1 <- as.matrix(tdm.2[,500]),dec=T))
