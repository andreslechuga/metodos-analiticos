sim900 %>%
arrange(word_id) %>%
group_by(word) %>%
summarise(aparece_en=n())
head(mat.hashed)
dim(mat.hashed)
dim(firmas.b)
split(data.frame((firmas.b)), rep(1:8, each=2))
mejores20
filter(compara,aparece_en ==2)
nrow(comapra)
nrow(compara)
as.data.frame(clus1)
as.data.frame(clus2)
as.data.frame(clus1)[1:50,]
datos_fechas <- dat.2
datos_fechas$caso <- 1:nrow(datos_fechas)
enero_00 <- tail(filter(datos_fechas,fecha=="2000-01-31"),n=1)$caso
lista <- list()
c <- 0.001
for(i in 1:enero_00){
dato <- vistas[i]
peli <- as.character(dato)
valor.peli <- lista[[peli]]
lista <- lapply(lista, function(x){ x*(1-c) })
if(is.null(valor.peli)){
lista[[peli]] <- c
} else {
lista[[peli]] <- valor.peli + c
}
lista <- Filter( function(x){ x >= c/2}, lista)
}
#hasta enero del 2000, las peliculas mas populares son
peli.scores_01_2000 <- data.frame(peli_id = as.integer(names(lista)), score= Reduce('c', lista)) %>%
left_join(pelis.nombres) %>% arrange(desc(score))
peli.scores_01_2000
# las peliculas mas populares en junio 2000 son
datos_fechas <- dat.2
datos_fechas$caso <- 1:nrow(datos_fechas)
junio <- tail(filter(datos_fechas,fecha=="2000-06-30"),n=1)$caso
lista <- list()
c <- 0.001
for(i in 1:junio){
dato <- vistas[i]
peli <- as.character(dato)
valor.peli <- lista[[peli]]
lista <- lapply(lista, function(x){ x*(1-c) })
if(is.null(valor.peli)){
lista[[peli]] <- c
} else {
lista[[peli]] <- valor.peli + c
}
lista <- Filter( function(x){ x >= c/2}, lista)
}
peli.scores_06_2000 <- data.frame(peli_id = as.integer(names(lista)), score= Reduce('c', lista)) %>%
left_join(pelis.nombres) %>% arrange(desc(score))
peli.scores_06_2000
#hasta enero del 2001, las peliculas mas populares son
datos_fechas <- dat.2
datos_fechas$caso <- 1:nrow(datos_fechas)
enero_01 <- tail(filter(datos_fechas,fecha=="2001-01-30"),n=1)$caso
lista <- list()
c <- 0.001
for(i in 1:enero_01){
dato <- vistas[i]
peli <- as.character(dato)
valor.peli <- lista[[peli]]
lista <- lapply(lista, function(x){ x*(1-c) })
if(is.null(valor.peli)){
lista[[peli]] <- c
} else {
lista[[peli]] <- valor.peli + c
}
lista <- Filter( function(x){ x >= c/2}, lista)
}
peli.scores_01_2001 <- data.frame(peli_id = as.integer(names(lista)), score= Reduce('c', lista)) %>%
left_join(pelis.nombres) %>% arrange(desc(score))
peli.scores_01_2001
save.image("~/Dropbox/ITAM_Master/Metodos_Analiticos/Primer_parcial/datos_parcial.RData")
head(fac.pers)
head(factores.peliculas)
factores.peliculas %>%
arrange(X4) %>%
select(movie_nom,tipo)
factores.peliculas %>%
arrange(X4) %>%
select(movie_nom,tipo) %>%
filter(row_number(tipo)<=10)
factores.peliculas %>%
arrange(X2) %>%
select(movie_nom,tipo) %>%
filter(row_number(tipo)<=10)
caso_6000[1:15,c(3,4)]
caso_6000
head(caso_6000)
head(dat.pred)
peli.scores_01_2000
head(peli.scores_01_2000)
peli.scores_01_2000[1:15,]
clus1
install.packages("wordcloud")
wordcloud(clus1$V1,clus1$n_tot,
scale=c(4,.5),
random.order=FALSE,
rot.per=.1,
colors=colorRampPalette(brewer.pal(9,"Set3"))(length(clus1$V1)),
ordered.colors=TRUE)
library(wordcloud)
wordcloud(clus1$V1,clus1$n_tot,
scale=c(4,.5),
random.order=FALSE,
rot.per=.1,
colors=colorRampPalette(brewer.pal(9,"Set3"))(length(clus1$V1)),
ordered.colors=TRUE)
clus1_wc <- as.data.frame(clus1)[1:50,]
wordcloud(clus1_wc$V1,clus1_wc$n_tot,
scale=c(4,.5),
random.order=FALSE,
rot.per=.1,
colors=colorRampPalette(brewer.pal(9,"Set3"))(length(clus1_wc$V1)),
ordered.colors=TRUE)
wordcloud(clus1_wc$V1,clus1_wc$n_tot,
scale=c(4,.5),
random.order=FALSE,
rot.per=.1,
colors=colorRampPalette(brewer.pal(9,"Set1"))(length(clus1_wc$V1)),
ordered.colors=TRUE)
clus2_wc <- as.data.frame(clus2)[1:50,]
wordcloud(clus2_wc$V1,clus2_wc$n_tot,
scale=c(4,.5),
random.order=FALSE,
rot.per=.1,
colors=colorRampPalette(brewer.pal(9,"Set1"))(length(clus2_wc$V1)),
ordered.colors=TRUE)
as.data.frame(clus2)[1:50,]
clus3_wc <- as.data.frame(clus3)[1:50,]
wordcloud(clus3_wc$V1,clus3_wc$n_tot,
scale=c(4,.5),
random.order=FALSE,
rot.per=.1,
colors=colorRampPalette(brewer.pal(9,"Set1"))(length(clus3_wc$V1)),
ordered.colors=TRUE)
class(caso_6000[1:15,c(3,4)])
caso_6000[1:15,c(3,4)]
Hoja1.Table.1 <- read.csv("~/Documents/analisis_tesis/PIB_SALUD_MARG_REC/Hoja1-Table 1.csv")
View(Hoja1.Table.1)
datos <- read.csv("~/Documents/analisis_tesis/PIB_SALUD_MARG_REC/Hoja1-Table 1.csv")
rm(Hoja1.Table.1)
colnames(datos)
colnames(datos) <- c("Year","PIB (2008)", "Salud (2008)", "Marg (2008)", "Rec (2008)")
head(datos)
colnames(datos) <- c("Year","PIB_2008", "Salud_2008", "Marg_2008", "Rec_2008")
head(datos)
ggplot(datos,aes(x=Year,y=PIB_2008))+
geom_line()
library(ggplot2)
ggplot(datos,aes(x=Year,y=PIB_2008))+
geom_line()
ggplot(datos)+
geom_line(aes(x=Year,y=PIB_2008))+
geom_line(aes(x=Year,y=Salud_2008))
scaled.dat[,-1]
datos[,-1]
scale(datos[,-1])
colMeans(scale(datos[,-1]))  # faster version of apply(scaled.dat, 2, mean)
apply(scale(datos[,-1]), 2, sd)
datos_sd <- cbind(datos$Year,scale(datos[,-1]))
head(datos_sd)
datos_sd <- cbind(Year=datos$Year,scale(datos[,-1]))
head(datos_sd)
datos$PIB_2008-mean(datos$PIB_2008)
(datos$PIB_2008-mean(datos$PIB_2008))/sd(datos$PIB_2008)
((datos$PIB_2008-mean(datos$PIB_2008))/sd(datos$PIB_2008))-datos_sd$PIB_2008
datos_sd$PIB_2008
datos_sd <- as.data.frame(cbind(Year=datos$Year,scale(datos[,-1])))
((datos$PIB_2008-mean(datos$PIB_2008))/sd(datos$PIB_2008))-datos_sd$PIB_2008
ggplot(datos_sd)+
geom_line(aes(x=Year,y=PIB_2008))+
geom_line(aes(x=Year,y=Salud_2008))
ggplot(datos_sd)+
geom_line(aes(x=Year,y=PIB_2008))+
geom_line(aes(x=Year,y=Salud_2008))+
geom_line(aes(x=Year,y=Marg_2008)) +
geom_line(aes(x=Year,y=Rec_2008))
ggplot(datos_sd)+
geom_line(aes(x=Year,y=PIB_2008),color=red)+
geom_line(aes(x=Year,y=Salud_2008))+
geom_line(aes(x=Year,y=Marg_2008)) +
geom_line(aes(x=Year,y=Rec_2008))
ggplot(datos_sd)+
geom_line(aes(x=Year,y=PIB_2008),color="red")+
geom_line(aes(x=Year,y=Salud_2008))+
geom_line(aes(x=Year,y=Marg_2008)) +
geom_line(aes(x=Year,y=Rec_2008))
ggplot(datos_sd)+
geom_line(aes(x=Year,y=PIB_2008),color="red")+
geom_line(aes(x=Year,y=Salud_2008),color="blue")+
geom_line(aes(x=Year,y=Marg_2008)) +
geom_line(aes(x=Year,y=Rec_2008))
ggplot(datos_sd)+
geom_line(aes(x=Year,y=PIB_2008),color="red")+
geom_line(aes(x=Year,y=Salud_2008),color="blue")+
geom_line(aes(x=Year,y=Marg_2008)) +
geom_line(aes(x=Year,y=Rec_2008),color="green")
datos_sd$Salud_2008
datos_sd[,c(1,3)]
ggplot(datos_sd)+
geom_line(aes(x=Year,y=PIB_2008),color="red")+
geom_line(aes(x=Year,y=Salud_2008),color="blue")+
geom_line(aes(x=Year,y=Marg_2008))
ggplot(datos)+
geom_line(aes(x=Year,y=Salud_2008),color="blue")
datos <- read.csv("~/Documents/analisis_tesis/PIB_SALUD_MARG_REC/Hoja1-Table 1.csv")
colnames(datos) <- c("Year","PIB_2008",
"Salud_2008","Marg_2008",
"Rec_2008")
ggplot(datos)+
geom_line(aes(x=Year,y=Salud_2008),color="blue")
datos <- read.csv("~/Documents/analisis_tesis/PIB_SALUD_MARG_REC/Hoja1-Table 1.csv")
head(datos)
colnames(datos) <- c("Year","PIB_2008",
"Salud_2008","Marg_2008",
"Rec_2008")
datos_sd <- as.data.frame(cbind(Year=datos$Year,scale(datos[,-1])))
ggplot(datos_sd)+
geom_line(aes(x=Year,y=PIB_2008),color="red")+
geom_line(aes(x=Year,y=Salud_2008),color="blue")+
geom_line(aes(x=Year,y=Marg_2008))
ggplot(datos_sd)+
geom_line(aes(x=Year,y=PIB_2008),color="red")+
geom_line(aes(x=Year,y=Salud_2008),color="blue")+
geom_line(aes(x=Year,y=Marg_2008)) +
geom_line(aes(x=Year,y=Rec_2008),color="green")
install.packages("urca")
library(urca)
data(zeroyld)
install.packages("tsDyn")
library(tsDyn)
data(zeroyld)
data<-zeroyld
#Fit a VECM with Engle-Granger 2OLS estimator:
vecm.eg<-VECM(zeroyld, lag=2)
vecm.eg
vecm.jo<-VECM(zeroyld, lag=2, estim="ML")
vecm.jo
if(require(vars)) {
data(finland)
#check long coint values
all.equal(VECM(finland, lag=2, estim="ML", r=2)$model.specific$beta,
cajorls(ca.jo(finland, K=3, spec="transitory"), r=2)  $beta, check.attributes=FALSE)
# check OLS parameters
all.equal(t(coefficients(VECM(finland, lag=2, estim="ML", r=2))),
coefficients(cajorls(ca.jo(finland, K=3, spec="transitory"), r=2)$rlm), check.attributes=FALSE)
}
summary(vecm.jo)
datos <- read.csv("~/Documents/analisis_tesis/PIB_SALUD_MARG_REC/Hoja1-Table 1.csv")
colnames(datos) <- c("Year","PIB_2008",
"Salud_2008","Marg_2008",
"Rec_2008")
head(datos)
datos_sd <- as.data.frame(cbind(Year=datos$Year,scale(datos[,-1])))
library(ggplot2)
library(urca)
library(tsDyn)
datos_sd <- as.data.frame(cbind(Year=datos$Year,scale(datos[,-1])))
ggplot(datos_sd)+
geom_line(aes(x=Year,y=PIB_2008),color="red")+
geom_line(aes(x=Year,y=Salud_2008),color="blue")+
geom_line(aes(x=Year,y=Marg_2008)) +
geom_line(aes(x=Year,y=Rec_2008),color="green")
ggplot(datos_sd)+
geom_line(aes(x=Year,y=PIB_2008),color="red")
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
load("~/Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/gutenberg_data_frame.Rdata")
library(Matrix)
library(dplyr)
library(tm)
library(ggplot2)
library(wordcloud)
################################################  lectura de la informacion ###########################################
#load('output/gutenberg_data_frame.Rdata')
View(dict_df)
###################################  limpieza de la informacion y creacion del corpus ##################################
d <- dict_df %>%
filter(grepl('^[a-z]', Word), id != 'Metadata', Def != '')
dim(d)
corpus.frases <- Corpus(VectorSource(d$Def))
corpus.frases
corp.1 <- tm_map(corpus.frases,  function(x){
gsub('--|[«»\\\',;:".!¡¿?\\(\\)\\[\\]&0-9\\*#/]|<br>','',x) # Checar escapes de &*[]
})
corp.2 <- tm_map(corp.1, function(x) stripWhitespace(x) %>% tolower %>% PlainTextDocument)
############################  Creacion de la matriz terminos documentos y los pesos por ntc ############################
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(1, Inf)))
tdm.2 <- weightSMART(tdm.1, spec = 'ntc')
colnames(tdm.2) <- paste(d$Word, d$id, sep='_')
#####################################  revisamos la normalizacion de los pesos #########################################
head(sort(vec.1 <- as.matrix(tdm.2[,'action_1']),dec=T))
########################################################  Query #######################################################
query <- 'i would like to go to the green park and spend a great time with my friends'
query <- 'wine'
query <- 'corazón'
d[which(d$Word %in% query)[1],]$Def
d[which(d$Word %in% query)[1],]$Def==NA
if(d[which(d$Word %in% query)[1],]$Def==NA){a <- 3}
if(is.na(d[which(d$Word %in% query)[1],]$Def)){a <- 3}
a
query <- 'corazón'
if(sapply(gregexpr("\\W+", query), length) + 1 == 2) {
#si la longitud del query es de 1 entonces, buscamos y limpiamos su definicion
if(is.na(d[which(d$Word %in% query)[1],]$Def)){
aux <- tm_map(Corpus(VectorSource(query)),removeWords,stopwords("english"))
}else{
definicion <- paste(query,d[which(d$Word %in% query)[1],]$Def)
definicion <- gsub('--|[],;:.[]|<br>|[()«»"#*`¿?¡!/&%$=]','',definicion)
definicion <- tm_map(Corpus(VectorSource(definicion)), function(x) stripWhitespace(x) %>% tolower %>% PlainTextDocument)
definicion <- tm_map(definicion,removeWords,stopwords("english"))
aux <- definicion
}
}else{
aux <- tm_map(Corpus(VectorSource(query)),removeWords,stopwords("english"))
}
aux[[1]]
query <- 'car'
if(sapply(gregexpr("\\W+", query), length) + 1 == 2) {
#si la longitud del query es de 1 entonces, buscamos y limpiamos su definicion
if(is.na(d[which(d$Word %in% query)[1],]$Def)){
aux <- tm_map(Corpus(VectorSource(query)),removeWords,stopwords("english"))
}else{
definicion <- paste(query,d[which(d$Word %in% query)[1],]$Def)
definicion <- gsub('--|[],;:.[]|<br>|[()«»"#*`¿?¡!/&%$=]','',definicion)
definicion <- tm_map(Corpus(VectorSource(definicion)), function(x) stripWhitespace(x) %>% tolower %>% PlainTextDocument)
definicion <- tm_map(definicion,removeWords,stopwords("english"))
aux <- definicion
}
}else{
aux <- tm_map(Corpus(VectorSource(query)),removeWords,stopwords("english"))
}
aux[[1]]
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
query <- 'car'
if(sapply(gregexpr("\\W+", query), length) + 1 == 2) {
#si la longitud del query es de 1 entonces, buscamos y limpiamos su definicion
if(is.na(d[which(d$Word %in% query)[1],]$Def)){
aux <- tm_map(Corpus(VectorSource(query)),removeWords,stopwords("english"))
}else{
definicion <- paste(query,d[which(d$Word %in% query)[1],]$Def)
definicion <- gsub('--|[],;:.[]|<br>|[()«»"#*`¿?¡!/&%$=]','',definicion)
definicion <- tm_map(Corpus(VectorSource(definicion)), function(x) stripWhitespace(x) %>% tolower %>% PlainTextDocument)
definicion <- tm_map(definicion,removeWords,stopwords("english"))
aux <- definicion
}
}else{
aux <- tm_map(Corpus(VectorSource(query)),removeWords,stopwords("english"))
}
################################################ Multiplicacion query * tdm ############################################
mat.1 <- sparseMatrix(i=tdm.2$i, j=tdm.2$j, x = tdm.2$v)
dictionary <- tdm.2$dimnames$Terms
query.vec.1 <- TermDocumentMatrix(aux,
control = list(dictionary = dictionary,
wordLengths=c(1, Inf)))
query.vec.norm <- as.matrix(query.vec.1)/sqrt(sum(query.vec.1^2))
aa <- t(mat.1)%*%query.vec.norm
###################################################  top 15 palabras ##################################################
idx_top <- order(aa, decreasing=T)
out <- d[idx_top,] %>%
select(Word, id, Def) %>%
cbind(score = sort(aa,decreasing = T)) %>%
filter(score > 0)
res <- out %>% head(15)
res$Def <- gsub('<br>','',res$Def)
View(res)
##############################################  histograma de discriminacion ##########################################
m <- data.frame(min=min(res$score))
# Estadísticas. Son sobresalientes las palabras que mostramos?
ggplot() +
geom_bar(data=out, mapping=aes(x=score)) +
geom_vline(data=res, aes(xintercept=min(score)), color='red')
############################################### contribucion de las palabras #########################################
best <- function(nmatch = 3, nterm = 5){
v.q <- query.vec.norm
outlist <- list()
for(i in 1:nmatch){
#colnames(tdm.2)[idx_top[i]]
#idx_top[nmatch]
v.j <- mat.1[,idx_top[i]]
v <- v.j*v.q
#length(v)
top_contrib <- order(v, decreasing = T)
outlist[[i]] <- data.frame(term=dictionary[top_contrib[1:nterm]], # tdm.2$dimnames$Terms[top_contrib[1]]
score_contrib=v[top_contrib[1:nterm]], stringsAsFactors=F) %>%
filter(score_contrib > 0) %>%
data.frame(rank = i, match = colnames(tdm.2)[idx_top[i]], total_score = sum(v), stringsAsFactors = F)
}
rbind_all(outlist)[c(3,4,5,1,2)] %>%
group_by(term) %>%
summarise(contrib=sum(score_contrib)) %>%
arrange(desc(contrib))
}
best <- best(nmatch = 15, nterm = nrow(unique(as.data.frame(strsplit(as.character(aux[[1]])," ")[[1]]))))
########################################################  wordcloud ##################################################
wordcloud(best$term,best$contrib,
scale=c(5,.7),
min.freq=0.1,
ordered.colors=T,
colors=colorRampPalette(brewer.pal(9,"Set1"))(nrow(best)))
best
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
shiny::runApp('Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Pimp_my_query/App_Shiny')
best
best$contrib
b <- as.data.frame(best$contrib)
b
norm(b)
b/max(b)
b/max(b)*10
setwd("~/Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Abstracts/App_Shiny")
load('data/d.Rdata')
library(Matrix)
library(dplyr)
library(tm)
library(slam)
library(Rstem)
library(ggplot2)
library(wordcloud)
setwd("~/Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Abstracts")
load('abstracts_clean.Rdata')
abstracts2 <- as.data.frame(abstracts2)
d <- abstracts2 %>%
filter(grepl('Presidential Awardee',Title)=='FALSE') %>% #815
filter(grepl('Not Available',Abstract)=='FALSE') %>% #1267
filter(Title != '') %>% #8
filter(Abstract != '' ) %>% #2180
filter(grepl('-----------------------------------------------------------------------',Abstract)=='FALSE')
#save(d,file='App_Shiny/data/d.Rdata')
corpus.frases <- Corpus(VectorSource(d$Abstract))
corp.1 <- tm_map(corpus.frases,function(x){
c1 <- gsub('R o o t E n t<br> r y','Root Entry',x)
c2 <- gsub('C o m p O b j','Comp Obj',c1)
c3 <- gsub('S u m m a r y I n f o r m a t i o n','Summary Information',c2)
c4 <- gsub('<br> b <br>','',c3)
c5 <- gsub('W o r d D o c u m e n t','Word Document',c4)
c6 <- gsub('O b j e c t P o o l','Object Pool',c5)
c7 <- gsub('[-]|<br>',' ',c6)
gsub('[()]|[.,;:`"*#&/><]|[\\\']|[]\\[]','',c7)
})
corp.2 <- tm_map(corp.1,removeWords,stopwords("english"))
corp.2 <- tm_map(corp.2, function(x) stripWhitespace(x) %>%
tolower %>%
PlainTextDocument)
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(3, Inf)))
idx_sum <- as.numeric(as.matrix(rollup(tdm.1, 1, na.rm=TRUE, FUN = sum)))
tdm_new <- tdm.1[,idx_sum>0]
tdm.2 <- weightSMART(tdm_new, spec = 'ntc')
d <- tdm.2$dimnames$Terms
head(d)
tail(d)
d[500:510]
d[5000:5100]
d[20000]
nrow(d)
length(d)
d[50000:51000]
setwd("~/Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Abstracts")
save(d,file='App_Shiny/data/d.Rdata')
setwd("~/Dropbox/ITAM_Master/Metodos_Analiticos/Final_MA/omar/Abstracts")
load('abstracts_clean.Rdata')
abstracts2 <- as.data.frame(abstracts2)
d <- abstracts2 %>%
filter(grepl('Presidential Awardee',Title)=='FALSE') %>% #815
filter(grepl('Not Available',Abstract)=='FALSE') %>% #1267
filter(Title != '') %>% #8
filter(Abstract != '' ) %>% #2180
filter(grepl('-----------------------------------------------------------------------',Abstract)=='FALSE')
corpus.frases <- Corpus(VectorSource(d$Abstract))
corp.1 <- tm_map(corpus.frases,function(x){
c1 <- gsub('R o o t E n t<br> r y','Root Entry',x)
c2 <- gsub('C o m p O b j','Comp Obj',c1)
c3 <- gsub('S u m m a r y I n f o r m a t i o n','Summary Information',c2)
c4 <- gsub('<br> b <br>','',c3)
c5 <- gsub('W o r d D o c u m e n t','Word Document',c4)
c6 <- gsub('O b j e c t P o o l','Object Pool',c5)
c7 <- gsub('[-]|<br>',' ',c6)
gsub('[()]|[.,;:`"*#&/><]|[\\\']|[]\\[]','',c7)
})
corp.2 <- tm_map(corp.1,removeWords,stopwords("english"))
corp.2 <- tm_map(corp.2, function(x) stripWhitespace(x) %>%
tolower %>%
PlainTextDocument)
tdm.1 <- TermDocumentMatrix(corp.2, control=list(wordLengths=c(3, Inf)))
#eliminamos los documentos que no tienen terminos (empty docs)
idx_sum <- as.numeric(as.matrix(rollup(tdm.1, 1, na.rm=TRUE, FUN = sum)))
tdm_new <- tdm.1[,idx_sum>0]
#actualizamos los pesos
tdm.2 <- weightSMART(tdm_new, spec = 'ntc')
mat.1 <- sparseMatrix(i=tdm.2$i, j=tdm.2$j, x = tdm.2$v)
d <- tdm.2$dimnames$Terms
save(d,file='App_Shiny/data/d_sin_stem.Rdata')
d_sin_stem <- tdm.2$dimnames$Terms
save(d_sin_stem,file='App_Shiny/data/d_sin_stem.Rdata')
